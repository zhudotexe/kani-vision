{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kani-Vision Quickstart\n",
    "This colab notebook runs through the quickstart example found [here](https://github.com/zhudotexe/kani-vision/blob/main/examples/gpt-vision.py).\n",
    "\n",
    "Feel free to make a copy of this notebook and modify the code cells to run other examples!"
   ],
   "metadata": {
    "id": "eQIkAIRNMxOl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install kani-vision\n",
    "First we'll install the library. kani-vision requires Python 3.10+."
   ],
   "metadata": {
    "id": "ae89bBMWM_fX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python --version\n",
    "# for the latest development version:\n",
    "!pip install -q 'kani-vision[openai] @ git+https://github.com/zhudotexe/kani-vision.git@main'\n",
    "# for the stable version:\n",
    "# !pip install -q 'kani-vision[openai]'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzJhGTAYLneA",
    "outputId": "9c8ec193-4ff1-48ef-b894-a09015367634"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "Then, import all the necessary components. Note that instead of using kani's default `chat_in_terminal`, we're using a vision-specific `chat_in_terminal_vision`."
   ],
   "metadata": {
    "id": "j4kWNrq8NGQN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "g8gn1WknKsSD",
    "outputId": "151aa67b-18dd-4216-a678-b6f1a3235397"
   },
   "outputs": [],
   "source": [
    "from kani import Kani\n",
    "from kani.ext.vision import ImagePart, chat_in_terminal_vision\n",
    "from kani.ext.vision.engines.openai import OpenAIVisionEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OpenAI Key\n",
    "To use the OpenAIVisionEngine, you need your OpenAI API key. You can find it here: https://platform.openai.com/account/api-keys"
   ],
   "metadata": {
    "id": "k9m_ytjXP_xy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Insert your OpenAI API key (https://platform.openai.com/account/api-keys)\n",
    "api_key = \"sk-...\"  # @param {type:\"string\"}"
   ],
   "metadata": {
    "cellView": "form",
    "id": "QSP2oODLLWwL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kani\n",
    "Set up the kani engine and harness.\n",
    "\n",
    "kani uses an Engine to interact with the language model. You can specify other model parameters in the engine, like `temperature=0.7`, or change the model here.\n",
    "\n",
    "You can also try uncommenting the LLaVA code and using the LLaVA v1.5 engine! You'll likely need to change the Colab runtime to an A100 GPU runtime if so.\n",
    "\n",
    "The kani manages the chat state, prompting, and function calling. Here, we only give it the engine to call\n",
    "ChatGPT, but you can specify other parameters like `system_prompt=\"You are...\"` in the kani."
   ],
   "metadata": {
    "id": "13re5suiQL2f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# uncomment the next few lines to use LLaVA\n",
    "# !pip install -q 'kani-vision[llava]' bitsandbytes accelerate\n",
    "# !pip install --no-deps \"llava @ git+https://github.com/haotian-liu/LLaVA.git@v1.1.1\"\n",
    "# import torch\n",
    "# from kani.ext.vision.engines.llava import LlavaEngine\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# engine = LlavaEngine(\n",
    "#     \"liuhaotian/llava-v1.5-7b\",\n",
    "#     model_load_kwargs={\n",
    "#         \"device_map\": \"auto\",\n",
    "#         \"load_in_4bit\": True,\n",
    "#         \"low_cpu_mem_usage\": True,\n",
    "#         \"quantization_config\": BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_compute_dtype=torch.float16,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type='nf4'\n",
    "#         )\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# comment the next line if using LLaVA\n",
    "engine = OpenAIVisionEngine(api_key, model=\"gpt-4-vision-preview\")\n",
    "ai = Kani(engine)"
   ],
   "metadata": {
    "id": "Cb_fBh4JLY_5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Programmatic Example\n",
    "Now, you can query the kani with images by sending a prompt that's a list of two items: the string that is your query, and an `ImagePart` containing the image to send. You can provide these in any order, and even provide multiple strings or image parts in a single query!\n",
    "\n",
    "To construct an `ImagePart`, use the `ImagePart.from_path`, `ImagePart.from_bytes`, or `ImagePart.from_image` classmethods.\n",
    "\n",
    "Here, we'll download the kani-vision logo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "!wget -nc https://kani-vision.readthedocs.io/en/latest/_static/kani-vision-logo.png\n",
    "Image(\"kani-vision-logo.png\", width=300, height=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This line might show an error about await outside async function - you can ignore this\n",
    "msg = await ai.chat_round_str([\"Please describe this image:\", ImagePart.from_path(\"kani-vision-logo.png\")])\n",
    "print(msg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat\n",
    "Just like the base kani library, kani-vision comes with a utility to interact with a multimodal kani through your terminal!\n",
    "\n",
    "To send an image to the kani, prefix a file path or URL with an exclamation point, like `Describe this image: !path/to/file.png` or `Describe this image: !https://example.com/image.png`.\n",
    "\n",
    "You can end the chat by sending the message `!stop`."
   ],
   "metadata": {
    "id": "3j0UOmxCQgwC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "chat_in_terminal_vision(ai, stopword=\"!stop\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_FShlgILcmo",
    "outputId": "761f5f84-b83a-4097-8266-ed6f97aeff07"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
